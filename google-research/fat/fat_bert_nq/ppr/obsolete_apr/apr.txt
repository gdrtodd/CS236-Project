"""Backup (Obsolete) : APR implementation using pure python dictionary."""

from __future__ import absolute_import
from __future__ import division
from __future__ import google_type_annotations
from __future__ import print_function

from collections import Counter
import gzip
import json
import pickle

from absl import app
from absl import flags
import sling
import tensorflow as tf

FLAGS = flags.FLAGS


class ApproximatePageRank(object):

  def __init__(self, dump_data, full_wiki, files_dir, compressed=True):
    if not full_wiki and compressed:
      full_wiki = False
      ent2id_fp = files_dir + '/ent2id_sub.json.gz'
      id2ent_fp = files_dir + '/id2ent_sub.json.gz'
      all_facts_fp = files_dir + '/all_facts_sub.json.gz'
      rel_dict_fp = files_dir + '/rel_dict_sub.json.gz'
      kb_filename = None
      entnam_filename = files_dir + '/entity_names_sub.json.gz'
    elif full_wiki and compressed:
      full_wiki = True
      ent2id_fp = files_dir + '/ent2id_full.json.gz'
      id2ent_fp = files_dir + '/id2ent_full.json.gz'
      all_facts_fp = files_dir + '/all_facts_full.json.gz'
      rel_dict_fp = files_dir + '/rel_dict_full.json.gz'
      kb_filename = None
      entnam_filename = files_dir + '/entity_names_full.json.gz'
    elif not full_wiki:
      full_wiki = False
      ent2id_fp = files_dir + '/ent2id_sub.pkl'
      id2ent_fp = files_dir + '/id2ent_sub.pkl'
      all_facts_fp = files_dir + '/all_facts_sub.pkl'
      rel_dict_fp = files_dir + '/rel_dict_sub.pkl'
      kb_filename = files_dir + '/kb.sling'
      entnam_filename = files_dir + '/entity_names_sub.pkl'
    else:
      full_wiki = True
      ent2id_fp = files_dir + '/ent2id_full.pkl'
      id2ent_fp = files_dir + '/id2ent_full.pkl'
      all_facts_fp = files_dir + '/all_facts_full.pkl'
      rel_dict_fp = files_dir + '/rel_dict_full.pkl'
      kb_filename = files_dir + '/kb.sling'
      entnam_filename = files_dir + '/entity_names_full.pkl'

    if dump_data:
      self.dump_dicts(full_wiki, ent2id_fp, id2ent_fp, all_facts_fp,
                      rel_dict_fp, kb_filename)
    elif compressed:
      self.load_compressed_dicts(full_wiki, ent2id_fp, id2ent_fp, all_facts_fp,
                                 rel_dict_fp, kb_filename, entnam_filename)
    else:
      self.load_dicts(full_wiki, ent2id_fp, id2ent_fp, all_facts_fp,
                      rel_dict_fp, kb_filename, entnam_filename)

  def dump_dicts(self, full_wiki, ent2id_fp, id2ent_fp, all_facts_fp,
                 rel_dict_fp, kb_filename):
    self.kb = sling.Store()
    self.kb.load(kb_filename)
    self.ent2id = dict()
    self.all_facts = dict()
    self.rel_dict = dict()
    count = 0
    for x in self.kb:
      count += 1
      if not full_wiki:
        if count == 100000:
          break
      if 'isa' in self.kb[x.id] and self.kb[x.id]['isa']['id'] == '/w/item':
        subj = x.id
        for k, v in x:
          if 'isa' in k and k['isa']['id'] == '/w/property':
            if type(v) == sling.Frame and 'id' in v and v.id.startswith('Q'):
              obj = v.id
              rel = k.id
            elif type(v) == sling.Frame and 'is' in v and type(
                v['is']) == sling.Frame and 'id' in v['is'] and v['is'][
                    'id'].startswith('Q'):
              obj = v['is']['id']
              rel = k.id
            else:
              #    print(v, type(v))
              #    #obj = v
              continue
            if subj not in self.ent2id:
              self.ent2id[subj] = len(self.ent2id)
            if obj not in self.ent2id:
              self.ent2id[obj] = len(self.ent2id)
            subj_id = self.ent2id[subj]
            obj_id = self.ent2id[obj]
            # build graph
            if subj_id not in self.all_facts:
              self.all_facts[subj_id] = list()
            if obj_id not in self.all_facts:
              self.all_facts[obj_id] = list()
            self.all_facts[subj_id].append(obj_id)
            self.all_facts[obj_id].append(subj_id)
            if subj not in self.rel_dict:
              self.rel_dict[subj] = dict()
              self.rel_dict[subj][obj] = list()
            else:
              if obj not in self.rel_dict[subj]:
                self.rel_dict[subj][obj] = list()
            self.rel_dict[subj][obj].append(rel)
    self.id2ent = {idx: ent for ent, idx in self.ent2id.items()}
    print('%d entities loaded' % len(self.id2ent))
    fp1 = open(ent2id_fp, 'wb')
    pickle.dump(self.ent2id, fp1)
    fp2 = open(id2ent_fp, 'wb')
    pickle.dump(self.id2ent, fp2)
    fp3 = open(all_facts_fp, 'wb')
    pickle.dump(self.all_facts, fp3)
    fp4 = open(rel_dict_fp, 'wb')
    pickle.dump(self.rel_dict, fp4)

  def load_dicts(self, full_wiki, ent2id_fp, id2ent_fp, all_facts_fp,
                 rel_dict_fp, kb_filename, entnam_filename):
    fp1 = tf.gfile.Open(ent2id_fp, 'rb')
    self.ent2id = pickle.load(fp1)
    fp2 = tf.gfile.Open(id2ent_fp, 'rb')
    self.id2ent = pickle.load(fp2)
    fp3 = tf.gfile.Open(all_facts_fp, 'rb')
    self.all_facts = pickle.load(fp3)
    fp4 = tf.gfile.Open(rel_dict_fp, 'rb')
    self.rel_dict = pickle.load(fp4)
    fp5 = tf.gfile.Open(entname_filename, 'rb')
    self.kb = pickle.load(fp4)
    # self.kb = sling.Store()
    # self.kb.load(kb_filename)
    print('%d entities loaded' % len(self.id2ent))

  def load_compressed_dicts(self, full_wiki, ent2id_fp, id2ent_fp, all_facts_fp,
                            rel_dict_fp, kb_filename, entname_filename):
    tf.logging.info('Loading Compressed dicts')
    with gzip.GzipFile(fileobj=tf.gfile.Open(ent2id_fp, 'rb')) as fp1:
      self.ent2id = json.load(fp1)
    tf.logging.info('Loaded ent2id')
    with gzip.GzipFile(fileobj=tf.gfile.Open(id2ent_fp, 'rb')) as fp2:
      self.id2ent = json.load(fp2)
    tf.logging.info('Loaded id2ent')
    with gzip.GzipFile(fileobj=tf.gfile.Open(all_facts_fp, 'rb')) as fp3:
      self.all_facts = json.load(fp3)
    tf.logging.info('Loaded all facts')
    with gzip.GzipFile(fileobj=tf.gfile.Open(rel_dict_fp, 'rb')) as fp4:
      self.rel_dict = json.load(fp4)
    tf.logging.info('Loaded rel dict')
    with gzip.GzipFile(fileobj=tf.gfile.Open(entname_filename, 'rb')) as fp5:
      self.kb = json.load(fp5)
    # fp1 = tf.gfile.Open(ent2id_fp, 'rb')
    # self.ent2id = pickle.load(fp1)
    # fp2 = tf.gfile.Open(id2ent_fp, 'rb')
    # self.id2ent = pickle.load(fp2)
    # fp3 = tf.gfile.Open(all_facts_fp, 'rb')
    # self.all_facts = pickle.load(fp3)
    # fp4 = tf.gfile.Open(rel_dict_fp, 'rb')
    # self.rel_dict = pickle.load(fp4)

    # self.kb = sling.Store()
    # self.kb.load(kb_filename)
    print('%d entities loaded' % len(self.id2ent))

  def compute_apr(self, seeds, freq_dict, k, alpha, eps, weighted=True):
    if not seeds:
      return []
    apr_scores = dict()
    apr_residuals = self.get_residuals(seeds, freq_dict, weighted)

    all_under_threshold = False
    while not all_under_threshold:
      all_under_threshold = True
      for subj_id in list(apr_residuals):  #.keys():
        residual = apr_residuals[subj_id]
        if residual / len(self.all_facts[str(subj_id)]) > eps:
          all_under_threshold = False
          apr_scores[subj_id] = apr_scores.get(subj_id, 0) + alpha * residual
          apr_residuals[subj_id] = 0
          for obj_id in self.all_facts[str(subj_id)]:
            apr_residuals[obj_id] = apr_residuals.get(
                obj_id,
                0) + (1 - alpha) * residual / len(self.all_facts[str(subj_id)])

    apr_scores_counter = Counter(
        {ent_id: score for ent_id, score in apr_scores.items()})
    # top_k_entities = [(self.id2ent[ent_id], score) for ent_id, score in apr_scores_counter.most_common(k)]
    top_k_entities = apr_scores_counter.most_common(k)
    return top_k_entities

  def get_residuals(self, seeds, freq_dict, weighted):
    apr_residuals = dict()
    if weighted:
      total_weight = 0
      for k, v in freq_dict.items():
        total_weight += v
      for seed in seeds:
        if seed in self.ent2id:
          apr_residuals[self.ent2id[seed]] = float(
              freq_dict[seed]) / float(total_weight)
    else:
      for seed in seeds:
        if seed in self.ent2id:
          apr_residuals[self.ent2id[seed]] = 1.0 / len(seeds)
    return apr_residuals

  def get_subject_object_pair(self, seeds, freq_dict, top_k_entities,
                              MAX_DISTANCE):
    pairs = list()
    facts = list()
    top_k_entity_ids = set(ent for (ent, score) in top_k_entities)
    score_dict = dict()
    for (item, val) in top_k_entities:
      score_dict[item] = val
    # initialize
    subj_ids = set(self.ent2id[seed] for seed in seeds if seed in self.ent2id)
    #top_k_entity_ids = top_k_entity_ids - subj_ids
    prev_history = set()
    for i in range(MAX_DISTANCE):
      next_subj_ids = set()
      for subj_id in subj_ids:
        prev_history.update(set([subj_id]))
        obj_ids = set(self.all_facts[str(subj_id)]) & top_k_entity_ids
        #top_k_entity_ids = top_k_entity_ids - obj_ids
        pairs += [
            (subj_id, obj_id,
             min(score_dict.get(subj_id, -999),
                 score_dict.get(obj_id, -999)), (score_dict.get(subj_id, -999),
                                                 score_dict.get(obj_id, -999)))
            for obj_id in obj_ids
        ]
        obj_ids = obj_ids - prev_history
        next_subj_ids.update(obj_ids)
      subj_ids = next_subj_ids
    for (subj, obj, score, tup) in pairs:
      subj_ent = self.id2ent[str(subj)]
      obj_ent = self.id2ent[str(obj)]
      if subj_ent in self.rel_dict and obj_ent in self.rel_dict[subj_ent]:
        facts.extend([((subj_ent, self.kb[subj_ent]['name']),
                       (obj_ent, self.kb[obj_ent]['name']),
                       (x, self.kb[x]['name']), score, tup)
                      for x in self.rel_dict[subj_ent][obj_ent]])
      if obj_ent in self.rel_dict and subj_ent in self.rel_dict[obj_ent]:
        facts.extend([((obj_ent, self.kb[obj_ent]['name']),
                       (subj_ent, self.kb[subj_ent]['name']),
                       (x, self.kb[x]['name']), score, tup)
                      for x in self.rel_dict[obj_ent][subj_ent]])
    return pairs, facts


def main(argv):
  apr = ApproximatePageRank(dump_data=True, full_wiki=True)


if __name__ == '__main__':
  app.run(main)
