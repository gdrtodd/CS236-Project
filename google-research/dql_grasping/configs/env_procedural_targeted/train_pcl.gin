
# TRAIN-COLLECT-EVAL LOOP
train_collect_eval.policy_class = @ContinuousPCLPolicy
train_collect_eval.train_fn = @train_q
train_collect_eval.run_agent_fn = @run_agent_procedural
train_collect_eval.max_training_steps = 1000000

# CONFIGURE POLICY (TRAIN, RUN)
ContinuousPCLPolicy.q_func = @cnn_v0
ContinuousPCLPolicy.state_shape = (1, 64, 64, 3)
ContinuousPCLPolicy.num_actions = 4
ContinuousPCLPolicy.build_target = True

# TRAINING
get_data_v2.parse_fn = @parse_tfexample_sequence
parse_tfexample_sequence.img_size = 64
get_data_v2.batch_size = 64

parse_tfexample_sequence.action_size = 4
parse_tfexample_sequence.episode_length = 16

train_q.q_graph_fn = @discrete_pcl_graph

# 10k calibrated from training fixed environment 4 blocks.
train_q.training_steps = 1000
train_q.log_every_n_steps=1000
train_q.save_summaries_steps=1000
train_q.update_target_every_n_steps = 100
train_q.save_checkpoints_steps=1000
train_q.optimizer = @AdamOptimizer()

AdamOptimizer.learning_rate = 0.0001
huber_loss.delta = 1.0

# RUN AGENT
run_agent_procedural.num_episodes = 50
run_agent_procedural.num_greedy_train_episodes = 50
run_agent_procedural.num_greedy_test_episodes = 50
run_agent_procedural.episode_to_transitions_fn = @episode_to_transitions_v0

# CONFIGURE EXPLORATION
run_agent_procedural.explore_schedule = @LinearSchedule()
LinearSchedule.schedule_timesteps = 100000
LinearSchedule.final_p = 1.0

# DATA
run_agent.episode_to_transitions_fn = @episode_to_sequence_v0
episode_to_sequence_v0.continuous = True
episode_to_sequence_v0.episode_length = 16

discrete_pcl_graph.num_actions = 4
discrete_pcl_graph.clip_adv = 1.0
discrete_pcl_graph.gamma = 0.95
discrete_pcl_graph.continuous = True
discrete_pcl_graph.trust_pcl = True
