

# TRAIN-COLLECT-EVAL LOOP
train_collect_eval.policy_class = @InputActionContinuousDQNPolicy
train_collect_eval.train_fn = @train_q
train_collect_eval.run_agent_fn = @run_agent_procedural

# CONFIGURE POLICY (TRAIN, RUN)
InputActionContinuousDQNPolicy.q_func = @cnn_ia_v1
cnn_ia_v1.tstep = True
InputActionContinuousDQNPolicy.state_shape = (1, 64, 64, 3)
InputActionContinuousDQNPolicy.action_size = 4
InputActionContinuousDQNPolicy.include_tstep = True
InputActionContinuousDQNPolicy.build_target = False

# TRAINING
get_data_v2.parse_fn = @parse_tfexample_v0
parse_tfexample_v0.img_size = 64
parse_tfexample_v0.action_size = 4
parse_tfexample_v0.include_step = True

train_q.q_graph_fn = @random_continuous_q_graph
train_q.pack_transition_fn = @tstep_pack_transition_fn
train_q.training_steps = 1000
train_q.max_training_steps = 200000
train_q.log_every_n_steps=1000
train_q.save_summaries_steps=1000
train_q.update_target_every_n_steps = 50
train_q.save_checkpoints_steps=1000
train_q.optimizer = @AdamOptimizer()

random_continuous_q_graph.gamma = 0
random_continuous_q_graph.num_samples = 16
random_continuous_q_graph.target_network_type = 'doubleq'

AdamOptimizer.learning_rate = 0.001
huber_loss.delta = 1.0

# RUN AGENT
run_agent_procedural.num_episodes = 50 # collect
run_agent_procedural.num_greedy_train_episodes = 50 # eval
run_agent_procedural.num_greedy_test_episodes = 50 # eval
run_agent_procedural.explore_schedule = @LinearSchedule()
run_agent_procedural.episode_to_transitions_fn = @episode_to_transitions_mc
episode_to_transitions_mc.base_fn = @episode_to_transitions_v0

# CONFIGURE EXPLORATION
LinearSchedule.schedule_timesteps = 100000
LinearSchedule.final_p = .05

episode_to_transitions_v0.continuous = True
