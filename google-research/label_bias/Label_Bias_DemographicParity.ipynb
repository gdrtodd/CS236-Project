{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Label Bias - DemographicParity.ipynb",
      "version": "0.3.2",
      "provenance": [
        {
          "file_id": "1SSy4hzElo40ctyS_nfSYvEXBpO4eGbtl",
          "timestamp": 1566864117779
        }
      ],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    }
  },
  "cells": [
      {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN3Ek2S9IIwK",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xle-Myh5YUSk",
        "colab_type": "text"
      },
      "source": [
        "## Benchmark Fairness Experiments for Demographic Parity\n",
        "\n",
        "Requires paths to the appropriate datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpWFW27lIxjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cPickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt5ZpMw3JiP7",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_TTNGF7Fnaq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load Adult dataset\n",
        "\n",
        "CATEGORICAL_COLUMNS = [\n",
        "    'workclass', 'education', 'marital_status', 'occupation', 'relationship',\n",
        "    'race', 'gender', 'native_country'\n",
        "]\n",
        "CONTINUOUS_COLUMNS = [\n",
        "    'age', 'capital_gain', 'capital_loss', 'hours_per_week', 'education_num'\n",
        "]\n",
        "COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "    'income_bracket'\n",
        "]\n",
        "LABEL_COLUMN = 'label'\n",
        "\n",
        "PROTECTED_GROUPS = [\n",
        "    'gender_Female', 'gender_Male', 'race_White', 'race_Black'\n",
        "]\n",
        "\n",
        "\n",
        "def get_adult_data():\n",
        "  train_file = PATH_TO_ADULT_TRAIN_FILE\n",
        "  test_file = PATH_TO_ADULT_TEST_FILE\n",
        "\n",
        "  train_df_raw = pd.read_csv(train_file, names=COLUMNS, skipinitialspace=True)\n",
        "  test_df_raw = pd.read_csv(\n",
        "      test_file, names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
        "\n",
        "  train_df_raw[LABEL_COLUMN] = (\n",
        "      train_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
        "  test_df_raw[LABEL_COLUMN] = (\n",
        "      test_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
        "  # Preprocessing Features\n",
        "  pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "  # Functions for preprocessing categorical and continuous columns.\n",
        "  def binarize_categorical_columns(input_train_df,\n",
        "                                   input_test_df,\n",
        "                                   categorical_columns=[]):\n",
        "\n",
        "    def fix_columns(input_train_df, input_test_df):\n",
        "      test_df_missing_cols = set(input_train_df.columns) - set(\n",
        "          input_test_df.columns)\n",
        "      for c in test_df_missing_cols:\n",
        "        input_test_df[c] = 0\n",
        "      train_df_missing_cols = set(input_test_df.columns) - set(\n",
        "          input_train_df.columns)\n",
        "      for c in train_df_missing_cols:\n",
        "        input_train_df[c] = 0\n",
        "      input_train_df = input_train_df[input_test_df.columns]\n",
        "      return input_train_df, input_test_df\n",
        "\n",
        "    # Binarize categorical columns.\n",
        "    binarized_train_df = pd.get_dummies(\n",
        "        input_train_df, columns=categorical_columns)\n",
        "    binarized_test_df = pd.get_dummies(\n",
        "        input_test_df, columns=categorical_columns)\n",
        "    # Make sure the train and test dataframes have the same binarized columns.\n",
        "    fixed_train_df, fixed_test_df = fix_columns(binarized_train_df,\n",
        "                                                binarized_test_df)\n",
        "    return fixed_train_df, fixed_test_df\n",
        "  \n",
        "  def bucketize_continuous_column(input_train_df,\n",
        "                                  input_test_df,\n",
        "                                  continuous_column_name,\n",
        "                                  num_quantiles=None,\n",
        "                                  bins=None):\n",
        "    assert (num_quantiles is None or bins is None)\n",
        "    if num_quantiles is not None:\n",
        "      train_quantized, bins_quantized = pd.qcut(\n",
        "          input_train_df[continuous_column_name],\n",
        "          num_quantiles,\n",
        "          retbins=True,\n",
        "          labels=False)\n",
        "      input_train_df[continuous_column_name] = pd.cut(\n",
        "          input_train_df[continuous_column_name], bins_quantized, labels=False)\n",
        "      input_test_df[continuous_column_name] = pd.cut(\n",
        "          input_test_df[continuous_column_name], bins_quantized, labels=False)\n",
        "    elif bins is not None:\n",
        "      input_train_df[continuous_column_name] = pd.cut(\n",
        "          input_train_df[continuous_column_name], bins, labels=False)\n",
        "      input_test_df[continuous_column_name] = pd.cut(\n",
        "          input_test_df[continuous_column_name], bins, labels=False)\n",
        "\n",
        "  # Filter out all columns except the ones specified.\n",
        "  train_df = train_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS +\n",
        "                          [LABEL_COLUMN]]\n",
        "  test_df = test_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS +\n",
        "                        [LABEL_COLUMN]]\n",
        "  # Bucketize continuous columns.\n",
        "  bucketize_continuous_column(train_df, test_df, 'age', num_quantiles=4)\n",
        "  bucketize_continuous_column(\n",
        "      train_df, test_df, 'capital_gain', bins=[-1, 1, 4000, 10000, 100000])\n",
        "  bucketize_continuous_column(\n",
        "      train_df, test_df, 'capital_loss', bins=[-1, 1, 1800, 1950, 4500])\n",
        "  bucketize_continuous_column(\n",
        "      train_df, test_df, 'hours_per_week', bins=[0, 39, 41, 50, 100])\n",
        "  bucketize_continuous_column(\n",
        "      train_df, test_df, 'education_num', bins=[0, 8, 9, 11, 16])\n",
        "  train_df, test_df = binarize_categorical_columns(\n",
        "      train_df,\n",
        "      test_df,\n",
        "      categorical_columns=CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS)\n",
        "  feature_names = list(train_df.keys())\n",
        "  feature_names.remove(LABEL_COLUMN)\n",
        "  num_features = len(feature_names)\n",
        "  return train_df, test_df, feature_names\n",
        "\n",
        "\n",
        "train_df, test_df, feature_names = get_adult_data()\n",
        "X_train_adult = np.array(train_df[feature_names])\n",
        "y_train_adult = np.array(train_df[LABEL_COLUMN])\n",
        "X_test_adult = np.array(test_df[feature_names])\n",
        "y_test_adult = np.array(test_df[LABEL_COLUMN])\n",
        "\n",
        "protected_train_adult = [np.array(train_df[g]) for g in PROTECTED_GROUPS]\n",
        "protected_test_adult = [np.array(test_df[g]) for g in PROTECTED_GROUPS]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5T11e7ZjyP-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load Bank dataset\n",
        "\n",
        "FEATURES = [\n",
        "    u'campaign', u'previous', u'emp.var.rate', u'cons.price.idx',\n",
        "    u'cons.conf.idx', u'euribor3m', u'nr.employed', u'job_admin.',\n",
        "    u'job_blue-collar', u'job_entrepreneur', u'job_housemaid',\n",
        "    u'job_management', u'job_retired', u'job_self-employed', u'job_services',\n",
        "    u'job_student', u'job_technician', u'job_unemployed', u'job_unknown',\n",
        "    u'marital_divorced', u'marital_married', u'marital_single',\n",
        "    u'marital_unknown', u'education_basic.4y', u'education_basic.6y',\n",
        "    u'education_basic.9y', u'education_high.school', u'education_illiterate',\n",
        "    u'education_professional.course', u'education_university.degree',\n",
        "    u'education_unknown', u'default_no', u'default_unknown', u'default_yes',\n",
        "    u'housing_no', u'housing_unknown', u'housing_yes', u'loan_no',\n",
        "    u'loan_unknown', u'loan_yes', u'contact_cellular', u'contact_telephone',\n",
        "    u'day_of_week_fri', u'day_of_week_mon', u'day_of_week_thu',\n",
        "    u'day_of_week_tue', u'day_of_week_wed', u'poutcome_failure',\n",
        "    u'poutcome_nonexistent', u'poutcome_success', u'y_yes', u'age_0', u'age_1',\n",
        "    u'age_2', u'age_3', u'age_4', u'duration_0.0', u'duration_1.0',\n",
        "    u'duration_2.0', u'duration_3.0', u'duration_4.0'\n",
        "]\n",
        "features = [\n",
        "    u'campaign', u'previous', u'emp.var.rate', u'cons.price.idx',\n",
        "    u'cons.conf.idx', u'euribor3m', u'nr.employed', u'job_admin.',\n",
        "    u'job_blue-collar', u'job_entrepreneur', u'job_housemaid',\n",
        "    u'job_management', u'job_retired', u'job_self-employed', u'job_services',\n",
        "    u'job_student', u'job_technician', u'job_unemployed', u'job_unknown',\n",
        "    u'marital_divorced', u'marital_married', u'marital_single',\n",
        "    u'marital_unknown', u'education_basic.4y', u'education_basic.6y',\n",
        "    u'education_basic.9y', u'education_high.school', u'education_illiterate',\n",
        "    u'education_professional.course', u'education_university.degree',\n",
        "    u'education_unknown', u'default_no', u'default_unknown', u'default_yes',\n",
        "    u'housing_no', u'housing_unknown', u'housing_yes', u'loan_no',\n",
        "    u'loan_unknown', u'loan_yes', u'contact_cellular', u'contact_telephone',\n",
        "    u'day_of_week_fri', u'day_of_week_mon', u'day_of_week_thu',\n",
        "    u'day_of_week_tue', u'day_of_week_wed', u'poutcome_failure',\n",
        "    u'poutcome_nonexistent', u'poutcome_success', u'age_0', u'age_1',\n",
        "    u'age_2', u'age_3', u'age_4', u'duration_0.0', u'duration_1.0',\n",
        "    u'duration_2.0', u'duration_3.0', u'duration_4.0'\n",
        "] \n",
        "LABEL_COLUMN = [\"y_yes\"]\n",
        "protected_features = ['age_0', 'age_1', 'age_2', 'age_3', 'age_4']\n",
        "\n",
        "\n",
        "def get_data():\n",
        "  data_path = PATH_TO_BANK_DATA\n",
        "  df = pd.read_csv(data_file, sep=';')\n",
        "  continuous_features = [\n",
        "      'campaign', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
        "      'euribor3m', 'nr.employed'\n",
        "  ]\n",
        "  continuous_to_categorical_features = ['age', 'duration']\n",
        "  categorical_features = [\n",
        "      'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
        "      'day_of_week', 'poutcome', 'y'\n",
        "  ]\n",
        "\n",
        "  # Functions for preprocessing categorical and continuous columns.\n",
        "  def binarize_categorical_columns(input_df, categorical_columns=[]):\n",
        "    # Binarize categorical columns.\n",
        "    binarized_df = pd.get_dummies(input_df, columns=categorical_columns)\n",
        "    return binarized_df\n",
        "\n",
        "  def bucketize_continuous_column(input_df, continuous_column_name, bins=None):\n",
        "    input_df[continuous_column_name] = pd.cut(\n",
        "        input_df[continuous_column_name], bins, labels=False)\n",
        "\n",
        "  for c in continuous_to_categorical_features:\n",
        "    b = [0] + list(np.percentile(df[c], [20, 40, 60, 80, 100]))\n",
        "    bucketize_continuous_column(df, c, bins=b)\n",
        "\n",
        "  df = binarize_categorical_columns(\n",
        "      df,\n",
        "      categorical_columns=categorical_features +\n",
        "      continuous_to_categorical_features)\n",
        "\n",
        "  to_fill = [\n",
        "      u'duration_0.0', u'duration_1.0', u'duration_2.0', u'duration_3.0',\n",
        "      u'duration_4.0'\n",
        "  ]\n",
        "  for i in range(len(to_fill) - 1):\n",
        "    df[to_fill[i]] = df[to_fill[i:]].max(axis=1)\n",
        "\n",
        "  normalize_features = [\n",
        "      'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'\n",
        "  ]\n",
        "  for feature in normalize_features:\n",
        "    df[feature] = df[feature] - np.mean(df[feature])\n",
        "\n",
        "  label = [\"u'y_yes\"]\n",
        "  df = df[FEATURES]\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "df = get_data()\n",
        "\n",
        "y = np.array(df[LABEL_COLUMN]).flatten()\n",
        "\n",
        "X_train_bank, X_test_bank, y_train_bank, y_test_bank = train_test_split(df, y, test_size=0.2, random_state=42)\n",
        "protected_train_bank = [X_train_bank[g] for g in protected_features]\n",
        "protected_test_bank = [X_test_bank[g] for g in protected_features]\n",
        "X_train_bank = np.array(X_train_bank[features])\n",
        "X_test_bank = np.array(X_test_bank[features])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USb_xvfFnP_z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load COMPAS dataset\n",
        "\n",
        "LABEL_COLUMN = 'two_year_recid'\n",
        "PROTECTED_GROUPS = [\n",
        "    'sex_Female', 'sex_Male', 'race_Caucasian', 'race_African-American'\n",
        "]\n",
        "\n",
        "\n",
        "def get_data():\n",
        "  data_path = PATH_TO_COMPAS_DATA\n",
        "  df = pd.read_csv(data_path)\n",
        "  FEATURES = [\n",
        "      'age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex',\n",
        "      'priors_count', 'days_b_screening_arrest', 'decile_score', 'is_recid',\n",
        "      'two_year_recid'\n",
        "  ]\n",
        "  df = df[FEATURES]\n",
        "  df = df[df.days_b_screening_arrest <= 30]\n",
        "  df = df[df.days_b_screening_arrest >= -30]\n",
        "  df = df[df.is_recid != -1]\n",
        "  df = df[df.c_charge_degree != 'O']\n",
        "  df = df[df.score_text != 'N/A']\n",
        "  continuous_features = [\n",
        "      'priors_count', 'days_b_screening_arrest', 'is_recid', 'two_year_recid'\n",
        "  ]\n",
        "  continuous_to_categorical_features = ['age', 'decile_score', 'priors_count']\n",
        "  categorical_features = ['c_charge_degree', 'race', 'score_text', 'sex']\n",
        "\n",
        "  # Functions for preprocessing categorical and continuous columns.\n",
        "  def binarize_categorical_columns(input_df, categorical_columns=[]):\n",
        "    # Binarize categorical columns.\n",
        "    binarized_df = pd.get_dummies(input_df, columns=categorical_columns)\n",
        "    return binarized_df\n",
        "\n",
        "  def bucketize_continuous_column(input_df, continuous_column_name, bins=None):\n",
        "    input_df[continuous_column_name] = pd.cut(\n",
        "        input_df[continuous_column_name], bins, labels=False)\n",
        "\n",
        "  for c in continuous_to_categorical_features:\n",
        "    b = [0] + list(np.percentile(df[c], [20, 40, 60, 80, 90, 100]))\n",
        "    if c == 'priors_count':\n",
        "      b = list(np.percentile(df[c], [0, 50, 70, 80, 90, 100]))\n",
        "    bucketize_continuous_column(df, c, bins=b)\n",
        "\n",
        "  df = binarize_categorical_columns(\n",
        "      df,\n",
        "      categorical_columns=categorical_features +\n",
        "      continuous_to_categorical_features)\n",
        "\n",
        "  to_fill = [\n",
        "      u'decile_score_0', u'decile_score_1', u'decile_score_2',\n",
        "      u'decile_score_3', u'decile_score_4', u'decile_score_5'\n",
        "  ]\n",
        "  for i in range(len(to_fill) - 1):\n",
        "    df[to_fill[i]] = df[to_fill[i:]].max(axis=1)\n",
        "  to_fill = [\n",
        "      u'priors_count_0.0', u'priors_count_1.0', u'priors_count_2.0',\n",
        "      u'priors_count_3.0', u'priors_count_4.0'\n",
        "  ]\n",
        "  for i in range(len(to_fill) - 1):\n",
        "    df[to_fill[i]] = df[to_fill[i:]].max(axis=1)\n",
        "\n",
        "  features = [\n",
        "      u'days_b_screening_arrest', u'c_charge_degree_F', u'c_charge_degree_M',\n",
        "      u'race_African-American', u'race_Asian', u'race_Caucasian',\n",
        "      u'race_Hispanic', u'race_Native American', u'race_Other',\n",
        "      u'score_text_High', u'score_text_Low', u'score_text_Medium',\n",
        "      u'sex_Female', u'sex_Male', u'age_0', u'age_1', u'age_2', u'age_3',\n",
        "      u'age_4', u'age_5', u'decile_score_0', u'decile_score_1',\n",
        "      u'decile_score_2', u'decile_score_3', u'decile_score_4',\n",
        "      u'decile_score_5', u'priors_count_0.0', u'priors_count_1.0',\n",
        "      u'priors_count_2.0', u'priors_count_3.0', u'priors_count_4.0'\n",
        "  ]\n",
        "  label = ['two_year_recid']\n",
        "\n",
        "  df = df[features + label]\n",
        "  return df, features, label\n",
        "\n",
        "df, feature_names, label_column = get_data()\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df, random_state=12345)\n",
        "N = len(df)\n",
        "train_df = df[:int(N * 0.66)]\n",
        "test_df = df[int(N * 0.66):]\n",
        "\n",
        "X_train_compas = np.array(train_df[feature_names])\n",
        "y_train_compas = np.array(train_df[label_column]).flatten()\n",
        "X_test_compas = np.array(test_df[feature_names])\n",
        "y_test_compas = np.array(test_df[label_column]).flatten()\n",
        "\n",
        "protected_train_compas = [np.array(train_df[g]) for g in PROTECTED_GROUPS]\n",
        "protected_test_compas = [np.array(test_df[g]) for g in PROTECTED_GROUPS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mI2pv7Mqwms",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load Crime dataset\n",
        "\n",
        "LABEL_COLUMN = 'label'\n",
        "\n",
        "EXCLUDED_COLUMNS = [\n",
        "    'state', 'county', 'community', 'communityname', 'ViolentCrimesPerPop'\n",
        "]\n",
        "PROTECTED_GROUPS = ['racepctblack_cat_low', 'racepctblack_cat_high',\n",
        "                    'racePctAsian_cat_low', 'racePctAsian_cat_high',\n",
        "                    'racePctWhite_cat_low', 'racePctWhite_cat_high',\n",
        "                    'racePctHisp_cat_low','racePctHisp_cat_high']\n",
        "\n",
        "def _dataframes(data_dir):\n",
        "  \"\"\"Returns the dataframes and feature names.\"\"\"\n",
        "  train_file = os.path.join(data_dir, 'train.csv')\n",
        "  val_file = os.path.join(data_dir, 'val.csv')\n",
        "  test_file = os.path.join(data_dir, 'test.csv')\n",
        "\n",
        "  # Replace all missing feature values with the mean over the training set.\n",
        "  feature_names = [\n",
        "      name for name in train_df.keys()\n",
        "      if name not in [LABEL_COLUMN] + EXCLUDED_COLUMNS\n",
        "  ]\n",
        "  for column in feature_names:\n",
        "    train_mean = train_df[column].mean()\n",
        "    train_df[column].fillna(train_mean, inplace=True)\n",
        "    val_df[column].fillna(train_mean, inplace=True)\n",
        "    test_df[column].fillna(train_mean, inplace=True)\n",
        "\n",
        "  return train_df, val_df, test_df, feature_names\n",
        "\n",
        "train_df, val_df, test_df, feature_names = _dataframes(PATH_TO_CRIME_DATA)\n",
        "train_df = pd.concat((train_df, val_df))\n",
        "X_train_crime = np.array(train_df[feature_names])\n",
        "y_train_crime = np.array(train_df[LABEL_COLUMN]).flatten()\n",
        "X_test_crime = np.array(test_df[feature_names])\n",
        "y_test_crime = np.array(test_df[LABEL_COLUMN]).flatten()\n",
        "\n",
        "protected_train_crime = [np.array(train_df[g]) for g in PROTECTED_GROUPS]\n",
        "protected_test_crime = [np.array(test_df[g]) for g in PROTECTED_GROUPS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmsPQ6NrvXBu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load German Statlog dataset \n",
        "\n",
        "\n",
        "data_path = PATH_TO_GERMAN_STATLOG\n",
        "with open(data_path, \"rb\") as fp:\n",
        "    X = cPickle.load(fp)\n",
        "    y = cPickle.load(fp)\n",
        "\n",
        "# protected attribute is whether is age\n",
        "X_train_german, X_test_german, y_train_german, y_test_german = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "protected_train_german = [np.where(X_train_german[:, 9] <= 30, 1, 0)]\n",
        "protected_test_german = [np.where(X_test_german[:, 9] <= 30, 1, 0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYl7_Y188FsM",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WkOnsAuI4h8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dataset_names = [\"Adult\", \"Bank\", \"COMPAS\", \"Crime\", \"German Statlog\"]\n",
        "\n",
        "datas = [(X_train_adult, y_train_adult, X_test_adult, y_test_adult, protected_train_adult, protected_test_adult),\n",
        "         (X_train_bank, y_train_bank, X_test_bank, y_test_bank, protected_train_bank, protected_test_bank),\n",
        "         (X_train_compas, y_train_compas, X_test_compas, y_test_compas, protected_train_compas, protected_test_compas),\n",
        "         (X_train_crime, y_train_crime, X_test_crime, y_test_crime, protected_train_crime, protected_test_crime),\n",
        "         (X_train_german, y_train_german, X_test_german, y_test_german, protected_train_german, protected_test_german),]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmnRfsj6_Yz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Helper Functions\n",
        "def get_error_and_violations(y_pred, y, protected_attributes):\n",
        "  acc = np.mean(y_pred != y)\n",
        "  violations = []\n",
        "  for p in protected_attributes:\n",
        "    protected_idxs = np.where(p > 0)\n",
        "    violations.append(np.mean(y_pred) - np.mean(y_pred[protected_idxs]))\n",
        "  pairwise_violations = []\n",
        "  for i in range(len(protected_attributes)):\n",
        "    for j in range(i+1, len(protected_attributes)):\n",
        "      protected_idxs = np.where(np.logical_and(protected_attributes[i] > 0, protected_attributes[j] > 0))\n",
        "      if len(protected_idxs[0]) == 0:\n",
        "        continue\n",
        "      pairwise_violations.append(np.mean(y_pred) - np.mean(y_pred[protected_idxs]))\n",
        "  return acc, violations, pairwise_violations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figp3NbvS7QK",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression on original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIWtoX1mTEXb",
        "colab_type": "code",
        "outputId": "841aec87-b609-4422-acc9-fa0409a09da5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1539535437576,
          "user_tz": 420,
          "elapsed": 672,
          "user": {
            "displayName": "Heinrich Jiang",
            "photoUrl": "",
            "userId": "02010368581707572492"
          }
        },
        "cellView": "both",
        "colab": {
          "height": 734
        }
      },
      "source": [
        "#@title Run experiment\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "for dataset_idx, dataset_name in enumerate(dataset_names):\n",
        "  print(\"Processing \", dataset_name)\n",
        "  X_train, y_train, X_test, y_test, protected_train, protected_test = datas[dataset_idx]\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred_train = model.predict(X_train)\n",
        "  y_pred_test = model.predict(X_test)\n",
        "\n",
        "  acc, violations, pairwise_violations = get_error_and_violations(y_pred_train, y_train, protected_train)\n",
        "  print(\"Train Accuracy\", acc)\n",
        "  print(\"Train Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
        "  if len(pairwise_violations) > 0:\n",
        "    print(\"Train Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
        "\n",
        "  acc, violations, pairwise_violations = get_error_and_violations(y_pred_test, y_test, protected_test)\n",
        "  print(\"Test Accuracy\", acc)\n",
        "  print(\"Test Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
        "  if len(pairwise_violations) > 0:\n",
        "    print(\"Test Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing  Adult\n",
            "Train Accuracy 0.140229108443\n",
            "Train Violation 0.120911659631  \t\t All violations [0.12091165963123439, -0.059767759792933695, -0.014386216911587812, 0.11588009873795037]\n",
            "Train Intersect Violations 0.158277923277  \t All violations [0.11361360164680066, 0.1582779232765294, -0.072077592314821026, 0.073860584934578546]\n",
            "Test Accuracy 0.141514648977\n",
            "Test Violation 0.117349807699  \t\t All violations [0.11734980769902086, -0.058577652627660404, -0.013783644484174074, 0.11403163239645997]\n",
            "Test Intersect Violations 0.158158349955  \t All violations [0.11115215201775935, 0.15815834995501074, -0.07108345283695916, 0.072908589919246203]\n",
            "\n",
            "\n",
            "Processing  Bank\n",
            "Train Accuracy 0.0932018209408\n",
            "Train Violation 0.0369750067913  \t\t All violations [-0.030353485459843954, 0.012786318810394715, 0.024747736337771491, 0.034735388646452842, -0.036975006791313783]\n",
            "Test Accuracy 0.0941976207817\n",
            "Test Violation 0.0348620377928  \t\t All violations [-0.031402682670299276, 0.0099161707989539605, 0.029569348108718887, 0.029608005196939326, -0.034862037792751085]\n",
            "\n",
            "\n",
            "Processing  COMPAS\n",
            "Train Accuracy 0.308863245765\n",
            "Train Violation 0.188506966571  \t\t All violations [0.18850696657057997, -0.045432361534835097, 0.13119282766923007, -0.13762850038569835]\n",
            "Train Intersect Violations 0.210802544913  \t All violations [0.21080254491335595, 0.12743529487943922, 0.10710921572983068, -0.19365824572629647]\n",
            "Test Accuracy 0.314911862792\n",
            "Test Violation 0.204498421868  \t\t All violations [0.20449842186755596, -0.045788567928362389, 0.12165061113309267, -0.12910034994144315]\n",
            "Test Intersect Violations 0.26353718769  \t All violations [0.26353718769034074, 0.12354402997566405, 0.080890467322101423, -0.1808030161855414]\n",
            "\n",
            "\n",
            "Processing  Crime\n",
            "Train Accuracy 0.130434782609\n",
            "Train Violation 0.422994138879  \t\t All violations [0.21714307681324174, -0.37437386975373727, -0.031636528541465869, 0.008709443850636156, -0.42299413887877085, 0.2560663203586423, 0.061702751862934946, -0.149213427474297]\n",
            "Train Intersect Violations 0.516806020067  \t All violations [0.21505672503114959, 0.21796409487563911, -0.19635147461234415, 0.25819397993311038, 0.25819397993311038, 0.082137641904941383, -0.4189360649099389, -0.33722586739513388, -0.49871209741495587, -0.32469372060164897, -0.48413730841044794, -0.51680602006688958, 0.25436256230858934, 0.011545186367426741, -0.23160193843423654, -0.30813255067913453, 0.25819397993311038, 0.20980688315891682, -0.13195753521840475, -0.48807467678330757, -0.39035674470457077, 0.25492600607690125, 0.25819397993311038]\n",
            "Test Accuracy 0.11623246493\n",
            "Test Violation 0.421105067277  \t\t All violations [0.24043809544222136, -0.33824792442026913, -0.045390781563126203, 0.0039957215043584515, -0.42110506727741198, 0.29785246168011703, 0.061222975050630402, -0.17291887145076668]\n",
            "Test Intersect Violations 0.521477738085  \t All violations [0.2779425517702071, 0.16175207557973093, -0.2836260756807733, 0.30460921843687377, 0.30460921843687377, 0.054609218436873774, -0.41449190515863182, -0.29955744822979286, -0.46811805429039893, -0.2953907815631262, -0.41160699777934245, -0.51892019332783201, 0.29178870561636094, -0.0049145910869357601, -0.28998537615772085, -0.33718182633924559, 0.30460921843687377, 0.30460921843687377, -0.16907499208944199, -0.52147773808486542, -0.39632536100237853, 0.29408290264740011, 0.30460921843687377]\n",
            "\n",
            "\n",
            "Processing  German Statlog\n",
            "Train Accuracy 0.217910447761\n",
            "Train Violation 0.0767478397486  \t\t All violations [-0.076747839748625285]\n",
            "Test Accuracy 0.248484848485\n",
            "Test Violation 0.0766233766234  \t\t All violations [-0.076623376623376621]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaS909wgKpoM",
        "colab_type": "text"
      },
      "source": [
        "## Data debiasing procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeLAmk9OKvck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def debias_weights(original_labels, protected_attributes, multipliers):\n",
        "  exponents = np.zeros(len(original_labels))\n",
        "  for i, m in enumerate(multipliers):\n",
        "    exponents -= m * protected_attributes[i]\n",
        "  weights = np.exp(exponents)/ (np.exp(exponents) + np.exp(-exponents))\n",
        "  weights = np.where(original_labels > 0, 1 - weights, weights)\n",
        "  return weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPwShzeDIbLR",
        "colab_type": "text"
      },
      "source": [
        "## Our method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz-NnoqbEkbC",
        "colab_type": "code",
        "outputId": "e0d39b9d-5298-4203-a593-28413323eae8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1539535542714,
          "user_tz": 420,
          "elapsed": 45449,
          "user": {
            "displayName": "Heinrich Jiang",
            "photoUrl": "",
            "userId": "02010368581707572492"
          }
        },
        "colab": {
          "height": 836
        }
      },
      "source": [
        "for dataset_idx, dataset_name in enumerate(dataset_names):\n",
        "  print(\"Processing \", dataset_name)\n",
        "  X_train, y_train, X_test, y_test, protected_train, protected_test = datas[dataset_idx]\n",
        "  multipliers = np.zeros(len(protected_train))\n",
        "  learning_rate = 1.\n",
        "  n_iters = 100\n",
        "  for it in xrange(n_iters):\n",
        "    weights = debias_weights(y_train, protected_train, multipliers)\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    model.fit(X_train, y_train, weights)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    acc, violations, pairwise_violations = get_error_and_violations(y_pred_train, y_train, protected_train)\n",
        "    multipliers += learning_rate * np.array(violations)\n",
        "\n",
        "\n",
        "    if (it + 1) % n_iters == 0:\n",
        "      print(multipliers)\n",
        "      y_pred_test = model.predict(X_test)\n",
        "      acc, violations, pairwise_violations = get_error_and_violations(y_pred_train, y_train, protected_train)\n",
        "      print(\"Train Accuracy\", acc)\n",
        "      print(\"Train Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
        "      if len(pairwise_violations) > 0:\n",
        "        print(\"Train Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
        "\n",
        "      acc, violations, pairwise_violations = get_error_and_violations(y_pred_test, y_test, protected_test)\n",
        "      print(\"Test Accuracy\", acc)\n",
        "      print(\"Test Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
        "      if len(pairwise_violations) > 0:\n",
        "        print(\"Test Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
        "      print()\n",
        "      print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing  Adult\n",
            "[ 0.74370369 -0.36761966 -0.12284051  0.50342701]\n",
            "Train Accuracy 0.162495009367\n",
            "Train Violation 0.000627542725776  \t\t All violations [-4.5586933493796389e-05, 2.2534045922950607e-05, -0.00039453172279543813, -0.00062754272577569825]\n",
            "Train Intersect Violations 0.00452599923507  \t All violations [-0.0003685568185797794, -0.0045259992350741018, -0.00040623898900132849, 0.0032361283207246461]\n",
            "Test Accuracy 0.165161845096\n",
            "Test Violation 0.00372907097133  \t\t All violations [-0.0021920975784750496, 0.001094232133785733, 0.00044638998336021807, 0.0037290709713349857]\n",
            "Test Intersect Violations 0.00861296743273  \t All violations [-0.00077773218455212678, -0.0015115509952084161, 0.0010078140714572525, 0.0086129674327299899]\n",
            "\n",
            "\n",
            "Processing  Bank\n",
            "[-0.24251423  0.09385195  0.25537229  0.36994571 -0.44096364]\n",
            "Train Accuracy 0.0979969650986\n",
            "Train Violation 0.000132733819591  \t\t All violations [0.0001327338195911687, -6.1627455955774735e-05, -0.00010994342874444396, 6.1499165873384054e-05, -4.404668855070154e-05]\n",
            "Test Accuracy 0.0962612284535\n",
            "Test Violation 0.00560643267173  \t\t All violations [0.0017592506245516781, -0.0014750923264091054, 0.0020019206639847742, -0.0056064326717318244, 0.0029323907715318523]\n",
            "\n",
            "\n",
            "Processing  COMPAS\n",
            "[ 0.26616865 -0.06414973 -0.11836071 -0.5081252 ]\n",
            "Train Accuracy 0.353547753499\n",
            "Train Violation 0.0014945326179  \t\t All violations [0.0014945326179027774, -0.00036019966507042089, -0.000283535160735926, 0.00025152190170801836]\n",
            "Train Intersect Violations 0.0373803093543  \t All violations [-0.037380309354284325, 0.031301009327034363, 0.010939018376808107, -0.0063117843833203346]\n",
            "Test Accuracy 0.354454502144\n",
            "Test Violation 0.0154605958235  \t\t All violations [0.013136215658249961, -0.0029412867712932944, -0.014935818240055132, 0.015460595823539203]\n",
            "Test Intersect Violations 0.0488907201628  \t All violations [-0.023124332864956798, 0.048890720162754447, -0.012583481311447015, 0.0086192540063325385]\n",
            "\n",
            "\n",
            "Processing  Crime\n",
            "[ 0.4911687  -0.93575383 -0.02483419  0.11116091 -1.300904    0.42247919\n",
            "  0.12669718 -0.57351941]\n",
            "Train Accuracy 0.280267558528\n",
            "Train Violation 0.00166879288349  \t\t All violations [0.00085012438835527226, 0.00026392777595464316, 0.0005328496117000172, -0.0016687928834948108, -0.0011788469816881361, 6.4043264783320009e-05, 0.00017602534765006131, 0.0004142752935023461]\n",
            "Train Intersect Violations 0.0107023411371  \t All violations [-0.0010623647452291957, -0.00079191173643947295, 0.010702341137123745, 0.00082579792724720227, -0.0035325343077161116, 0.010702341137123745, 0.0017337312716528934, -0.0045648344353953381, -0.0031098135590088508, 0.0053547475542360451, -0.0015675975131830035, 0.00070234113712374507, -0.0046233293609605451, -0.0027024846001416705, 0.010702341137123745, -0.0046037813118558462, 0.010702341137123745, 0.010702341137123745, -0.00066129522651261867, 0.0032396545699595663, -0.00016722408026755876, -0.0056375281439220073, 0.010702341137123745]\n",
            "Test Accuracy 0.300601202405\n",
            "Test Violation 0.0106844705454  \t\t All violations [0.010684470545368811, -0.005945957849765468, 0.0010320641282565123, 0.0037621254779497629, -0.0068250787288863454, 0.0025185506147429974, 0.0054500535462459303, 0.0047961090720767367]\n",
            "Test Intersect Violations 0.0160320641283  \t All violations [0.016032064128256512, 0.016032064128256512, 0.016032064128256512, 0.0079675479992242539, 0.016032064128256512, 0.016032064128256512, -0.0064398459841030382, 0.016032064128256512, -0.014270966174773792, 0.0026987307949231775, 0.0025185506147429974, -0.013379700577625841, 0.0032115513077436916, 0.0080955561917485756, -0.010994962898770517, 0.016032064128256512, 0.016032064128256512, 0.016032064128256512, 0.0055057483387828277, -0.0057070663065260964, 0.0066862697357331479, 0.0055057483387828277, 0.016032064128256512]\n",
            "\n",
            "\n",
            "Processing  German Statlog\n",
            "[-0.32683949]\n",
            "Train Accuracy 0.238805970149\n",
            "Train Violation 0.00102120974077  \t\t All violations [-0.0010212097407698573]\n",
            "Test Accuracy 0.251515151515\n",
            "Test Violation 0.0137085137085  \t\t All violations [-0.013708513708513698]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLv2FHmXe5lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
